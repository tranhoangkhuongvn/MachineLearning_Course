{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning models training\n",
    "- Bias vs Variance\n",
    "- Underfitting vs Overfitting\n",
    "- Hyperparameters tuning\n",
    "- Machine Learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is one of many Supervised Machine Learning models that learn a mapping between inputs and outputs. After that, we can use the model to predict the output label for new or unseen data. The lower the differences between the predicted outputs and the ground truth labels, the better the model. However, there is a `No Free Lunch` theorm states that there is no model that can fit the dataset perfectly without any bias or assumption about the dataset which is called `inductive bias`. Each Machine Learning model that we are going to learn will have different set of assumptions about the dataset. For Linear Regression, the assumption was that all features and linearly correlated with the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias vs Variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bias` and `Variance` are two of the common source of errors that cause the difference between the predicted output and the true label. Let's take Regression as an example. The below illustration shows three possibilies that can happen when we try to train a model to fit the given dataset. If we make an assumption that the model is linear when in fact it is not, we are going to end up with `underfitting` the model where the line is not able to capture the underlying dataset in a non-linear way. The training loss when we have a `high bias` model which is when our assumption is far off from the dataset is large.\n",
    "<center><img src='./assets/under_over_fit.png' width=\"800\"></center>\n",
    "\n",
    "At the other end of the extreme, if we try to find a model (e.g makes it more complex) that fits every point in our dataset, in order to reduce the `bias error`. We are `overfitting` the data. Intuitively, it is similar to a student trying to learn by heart all the solutions of the prep questions before an exam without actually trying to understand the solutions. Then when the exams come with unseen questions, the students are unlikely to address those new questions. When the model overfits the data, it performs very well on the training loss which can go down to zero!. However it will not perform at all when coming to predict on new data. \n",
    "\n",
    "What we want is something in the middle where the model reasonably fits the data with some acceptable errors during the training. This would allow the model to predict properly on unseen data or we say that it is able to `generalise` on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These 2 sources of errors can not be fully eliminated at the same time. As data scientists, we need to find the balance in this `bias-variance tradeoff`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "- **Bias**: How close are predictions to the actual values?\n",
    "  - Roughly, whether or not our model aims on target.\n",
    "  - The model is not very sensitive to outliers or to variations in the dataset\n",
    "  - If the model cannot represent the data's structure, our predictions could be consistent, but will not be accurate.\n",
    "- **Variance**: How variable are our predictions?\n",
    "  - Roughly, whether or not our model is reliable.\n",
    "  - The model is extremely sensitive to outliers or to variations in the dataset\n",
    "  - It will make slightly different predictions given slightly different training sets.\n",
    "<center><img src='./assets/bulls_eye.png' width=\"600\"></center>\n",
    "- Normally simplified models have `high bias` while more complicated models will have `high variance`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can we find the right model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is we need to iteratively train the models on the data, inspect the loss and visualise it. This process can take up to hundreds of iterations in order to find the right parameters (e.g model complexity) in order to best capture the nature of the dataset.\n",
    "On the below figure, the horizontal axis is the model capacity or the complexity of the model. In the left corner closed to the origin, these denote the simpliest models, where the models only output a constant value for any input values or just a linear model of some features. And the model's complexity increases as we move along this axis (e.g high polynomial order models). The green line shows the changes in the variance error and the red line displays the bias error. The black line is the final error from the model when it is tested on unseen data. As you can see from this illustration, the generalisation error is the sum of the bias error and the variance error. As a result, to minimise the final error, we need to find the `sweet spot` where this trade-off can lead to the lowest test error.\n",
    "<center><img src='./assets/bias_variance.png' width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/ Test split:\n",
    "There are a few strategies and techniques that are used to find the right model complexity and reduces the overall test error. It is clear that we cannot select the right model from looking only at the training error. The whole purpose of the training process is the find the best model that fit the data, as a result, the training error tends to be low or goes as low as possible. However, test error is the one that tells us whether our model peforms. Therefore, one intuitive solution is to divide our dataset into 2 sets: one for training and the other for testing. \n",
    "\n",
    "The advantages of this method is in its simplicity of implementation, and it does the job of telling you how the model performs on both training and testing data. However, it only works well if we can split the dataset in such a way that both test and train data shares the same underlying distribution and are similar in nature. It also does not allow you to have iterations to tune the model's parameters. It is a good practice to have a train/ test split ratio of 80:20 or 70:30\n",
    "\n",
    "##### Train/ Validation/ Test split:\n",
    "This method is similar to previous train/test split. However it divides the dataset into an extra `validation set` which serves as a sample testbed for the model. After we train the model on the train dataset, we can test it on the validation data and uses the resulting loss to make appropriatet modifications to the model's parameters before re-training again. The test dataset is only used at the very end when we finalise the model. This method is ideal in the sense that it allows the model's designers plenty opportunity to fine-tune the parameters. However the disadvantage is that it requires a substantial amount of data in order to have enough data points in each subset. \n",
    "\n",
    "##### Cross validation:\n",
    "Cross validation is a method that can be used when the dataset is not large enough that you can separate into 3 subsets as the previous method. In this case, you can still divide the entire dataset into train and test data. We keep the test data aside and use cross-validation on the training set to simualate the validation set strategy. Then we divide our training data into k subsets (e.g k-folds) where k normally is 5. Let's denote them as $F_1, F_2, F_3, .. F_k$. For each training iteration, we are going to pick one of the subset to be the validation set, and we train the model with a specific set of parameters on the remaining folds. At the end, we can compare the performance between different runs to get a good estimate of how the model performs on different portions of the dataset. This method gives a much better evaluation metric on the model's performance. However, it does require extra computational cost in order to cross-validate the dataset this way. \n",
    "\n",
    "\n",
    "<center><img src='./assets/cross_validation_diagram.png' width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning modelling process:\n",
    "The previous lession on Linear Regression has outlined some important procedures when working with data science projects or when trying to apply ML in practice. Here is the general outline that is normally used in real-life projects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Frame the problem and clarify assumptions \\\n",
    "This step is neccessary to identify the problems and assumptions about the problem and the dataset. This is also where we decide whether we want to use ML in the project and establish a baseline to evaluate the model's performance later on. \n",
    "2. Get the data \\\n",
    "Collect the data for the model training, usually it involves identifying the type of information we want to have in the data\n",
    "3. Explore and visualise the data \\\n",
    "Try to find insights from the data through some summary statisics and plots\n",
    "4. Data transformations \\\n",
    "Encode the features in a way that is convenient for the model to learn (e.g feature scaling, one-hot encoding ...)\n",
    "5. Select the type of modeling and traing the model \\\n",
    "Regression, classification, decision tree or unsupervised learning etc...\n",
    "6. Fine-tune the hyperparameters \\\n",
    "Use one of the strategy above for finding the best model's hyperparameters\n",
    "7. Evaluate the model on the test data \\\n",
    "Run the model on the test data to see how it performs. This should be the first time the model runs on this data.\n",
    "8. Deploy/ launch the model and monitor its performance \\\n",
    "Congratulations, you have a decent model that performs well on both train and test data. Let's run it on production. But please remember to monitor its performance and re-train the model if needed on new incoming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
